{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple GAN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinkyukim-me/Summary-Seocho-Pytorch/blob/master/Simple_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kHnwY_DzxAk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "06507606-3ce9-4e4c-d4cb-f7fc9dfd8614"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated!')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "print('Mounted!')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authenticated!\n",
            "Mounted at /content/gdrive\n",
            "Mounted!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwAUzDyTz5sR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import Adam\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spXUrWoMz5qL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F9424asz5oD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist = datasets.MNIST(root='data', download=True, transform=transform)\n",
        "\n",
        "dataloader = DataLoader(mnist, batch_size=60, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7iWm2U6z5l0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import imageio\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    use_gpu = True\n",
        "    \n",
        "leave_log = True\n",
        "\n",
        "if leave_log:\n",
        "    result_dir = 'GAN_generated_images'\n",
        "    if not os.path.isdir(result_dir):\n",
        "        os.mkdir(result_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdfCz6MBz5jP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(in_features = 100, out_features = 256),\n",
        "            \n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(in_features = 256, out_features = 512),\n",
        "            \n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(in_features = 512, out_features = 1024),\n",
        "            \n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(in_features = 1024, out_features = 28*28),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.main(inputs).view(-1, 1, 28, 28)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_x-pWc7LI4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(in_features = 28*28, out_features = 1024),\n",
        "            \n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(inplace=True),\n",
        "            nn.Linear(in_features = 1024, out_features = 512),\n",
        "            \n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(inplace=True),\n",
        "            nn.Linear(in_features = 512, out_features = 256),\n",
        "            \n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(inplace=True),\n",
        "            nn.Linear(in_features = 256, out_features = 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        inputs = inputs.view(-1, 28 * 28)\n",
        "        return self.main(inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnPgj8XKMhzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D = Discriminator()\n",
        "G = Generator()\n",
        "\n",
        "if use_gpu:\n",
        "    G.cuda()\n",
        "    D.cuda()\n",
        "    \n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "D_optimizer = optim.Adam(D.parameters(), lr=0.0002)\n",
        "G_optimizer = optim.Adam(G.parameters(), lr=0.0002)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5MdcB4PNYYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "def square_plot(data, path):\n",
        "    \"\"\"Take an array of shape (n, height, width) or (n, height, width , 3)\n",
        "       and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)\"\"\"\n",
        "    \n",
        "    if type(data) == list:\n",
        "\t    data = np.concatenate(data)\n",
        "    # normalize data for display\n",
        "    data = (data - data.min()) / (data.max() - data.min())\n",
        "    \n",
        "    # force the number of filters to be square\n",
        "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
        "    \n",
        "    padding = (((0, n ** 2 - data.shape[0]) ,\n",
        "                (0, 1), (0, 1))  # add some space between filters\n",
        "               + ((0, 0),) * (data.ndim - 3))  # don't pad the last dimension (if there is one)\n",
        "    \n",
        "    data = np.pad(data , padding, mode='constant' , constant_values=1)  # pad with ones (white)\n",
        "    \n",
        "    # tilethe filters into an image\n",
        "    data = data.reshape((n , n) + data.shape[1:]).transpose((0 , 2 , 1 , 3) + tuple(range(4 , data.ndim + 1)))\n",
        "    data = data.reshape((n * data.shape[1] , n * data.shape[3]) + data.shape[4:])\n",
        "    plt.imsave(path, data, cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZdGoSgKOxTN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0a5257a6-35df-4b1a-cad2-8cbefebaff10"
      },
      "source": [
        "if leave_log:\n",
        "    train_hist = {}\n",
        "    train_hist['D_losses'] = []\n",
        "    train_hist['G_losses'] = []\n",
        "    generated_images = []\n",
        "\n",
        "z_fixed = Variable(torch.randn(5*5, 100), volatile=True)\n",
        "\n",
        "if use_gpu:\n",
        "    z_fixed = z_fixed.cuda()\n",
        "   "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWPxiVwNPOUA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "379e90c3-873d-4501-f406-c78859beb1e7"
      },
      "source": [
        "for epoch in range(100):\n",
        "    \n",
        "    if leave_log:\n",
        "        D_losses = []\n",
        "        G_losses = []\n",
        "    \n",
        "    for real_data, _ in dataloader:\n",
        "        batch_size = real_data.size(0)\n",
        "        \n",
        "\n",
        "        real_data = Variable(real_data)\n",
        "\n",
        "        target_real = Variable(torch.ones(batch_size, 1))\n",
        "        target_fake = Variable(torch.zeros(batch_size, 1))\n",
        "         \n",
        "        if use_gpu:\n",
        "            real_data, target_real, target_fake = real_data.cuda(), target_real.cuda(), target_fake.cuda()\n",
        "            \n",
        "\n",
        "        D_result_from_real = D(real_data)\n",
        "        D_loss_real = criterion(D_result_from_real, target_real)\n",
        "\n",
        "        z = Variable(torch.randn((batch_size, 100)))\n",
        "        \n",
        "        if use_gpu:\n",
        "            z = z.cuda()\n",
        "            \n",
        "\n",
        "        fake_data = G(z)\n",
        "        \n",
        "\n",
        "        D_result_from_fake = D(fake_data)\n",
        "        D_loss_fake = criterion(D_result_from_fake, target_fake)\n",
        "        \n",
        "\n",
        "        D_loss = D_loss_real + D_loss_fake\n",
        "        \n",
        "\n",
        "        D.zero_grad()\n",
        "\n",
        "        D_loss.backward()\n",
        "\n",
        "        D_optimizer.step()\n",
        "        \n",
        "        if leave_log:\n",
        "            D_losses.append(D_loss.item())\n",
        "\n",
        "        \n",
        "\n",
        "        z = Variable(torch.randn((batch_size, 100)))\n",
        "        \n",
        "        if use_gpu:\n",
        "            z = z.cuda()\n",
        "        \n",
        "\n",
        "        fake_data = G(z)\n",
        "\n",
        "        D_result_from_fake = D(fake_data)\n",
        "\n",
        "        G_loss = criterion(D_result_from_fake, target_real)\n",
        "        \n",
        "\n",
        "        G.zero_grad()\n",
        "\n",
        "        G_loss.backward()\n",
        "\n",
        "        G_optimizer.step()\n",
        "        \n",
        "        if leave_log:\n",
        "            G_losses.append(G_loss.item())\n",
        "            \n",
        "    if leave_log:\n",
        "        true_positive_rate = (D_result_from_real > 0.5).float().mean().item()\n",
        "        true_negative_rate = (D_result_from_fake < 0.5).float().mean().item()\n",
        "        base_message = (\"Epoch: {epoch:<3d} D Loss: {d_loss:<8.6} G Loss: {g_loss:<8.6} \"\n",
        "                        \"True Positive Rate: {tpr:<5.1%} True Negative Rate: {tnr:<5.1%}\"\n",
        "                       )\n",
        "        message = base_message.format(\n",
        "                    epoch=epoch,\n",
        "                    d_loss=sum(D_losses)/len(D_losses),\n",
        "                    g_loss=sum(G_losses)/len(G_losses),\n",
        "                    tpr=true_positive_rate,\n",
        "                    tnr=true_negative_rate\n",
        "        )\n",
        "        print(message)\n",
        "    \n",
        "    if leave_log:\n",
        "        fake_data_fixed = G(z_fixed)\n",
        "        image_path = result_dir + '/epoch{}.png'.format(epoch)\n",
        "        square_plot(fake_data_fixed.view(25, 28, 28).cpu().data.numpy(), path=image_path)\n",
        "        generated_images.append(image_path)\n",
        "    \n",
        "    if leave_log:\n",
        "        train_hist['D_losses'].append(torch.mean(torch.FloatTensor(D_losses)))\n",
        "        train_hist['G_losses'].append(torch.mean(torch.FloatTensor(G_losses)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0   D Loss: 0.946694 G Loss: 2.77823  True Positive Rate: 91.7% True Negative Rate: 83.3%\n",
            "Epoch: 1   D Loss: 0.739339 G Loss: 2.3576   True Positive Rate: 90.0% True Negative Rate: 96.7%\n",
            "Epoch: 2   D Loss: 0.539076 G Loss: 2.84856  True Positive Rate: 76.7% True Negative Rate: 88.3%\n",
            "Epoch: 3   D Loss: 0.678398 G Loss: 2.23925  True Positive Rate: 93.3% True Negative Rate: 95.0%\n",
            "Epoch: 4   D Loss: 0.705851 G Loss: 2.05034  True Positive Rate: 70.0% True Negative Rate: 95.0%\n",
            "Epoch: 5   D Loss: 0.824256 G Loss: 1.79897  True Positive Rate: 88.3% True Negative Rate: 95.0%\n",
            "Epoch: 6   D Loss: 0.881752 G Loss: 1.62285  True Positive Rate: 71.7% True Negative Rate: 83.3%\n",
            "Epoch: 7   D Loss: 0.911446 G Loss: 1.58236  True Positive Rate: 70.0% True Negative Rate: 91.7%\n",
            "Epoch: 8   D Loss: 0.945653 G Loss: 1.52441  True Positive Rate: 53.3% True Negative Rate: 81.7%\n",
            "Epoch: 9   D Loss: 0.978359 G Loss: 1.44012  True Positive Rate: 63.3% True Negative Rate: 80.0%\n",
            "Epoch: 10  D Loss: 0.982334 G Loss: 1.43404  True Positive Rate: 58.3% True Negative Rate: 88.3%\n",
            "Epoch: 11  D Loss: 1.00208  G Loss: 1.39465  True Positive Rate: 80.0% True Negative Rate: 85.0%\n",
            "Epoch: 12  D Loss: 1.04133  G Loss: 1.29878  True Positive Rate: 75.0% True Negative Rate: 80.0%\n",
            "Epoch: 13  D Loss: 1.09989  G Loss: 1.21527  True Positive Rate: 73.3% True Negative Rate: 86.7%\n",
            "Epoch: 14  D Loss: 1.08263  G Loss: 1.21483  True Positive Rate: 75.0% True Negative Rate: 83.3%\n",
            "Epoch: 15  D Loss: 1.10882  G Loss: 1.18026  True Positive Rate: 55.0% True Negative Rate: 86.7%\n",
            "Epoch: 16  D Loss: 1.11391  G Loss: 1.14317  True Positive Rate: 78.3% True Negative Rate: 81.7%\n",
            "Epoch: 17  D Loss: 1.14207  G Loss: 1.10788  True Positive Rate: 61.7% True Negative Rate: 61.7%\n",
            "Epoch: 18  D Loss: 1.13676  G Loss: 1.13089  True Positive Rate: 68.3% True Negative Rate: 61.7%\n",
            "Epoch: 19  D Loss: 1.12972  G Loss: 1.12704  True Positive Rate: 51.7% True Negative Rate: 90.0%\n",
            "Epoch: 20  D Loss: 1.14624  G Loss: 1.10415  True Positive Rate: 58.3% True Negative Rate: 71.7%\n",
            "Epoch: 21  D Loss: 1.1634   G Loss: 1.06593  True Positive Rate: 71.7% True Negative Rate: 83.3%\n",
            "Epoch: 22  D Loss: 1.17606  G Loss: 1.04972  True Positive Rate: 58.3% True Negative Rate: 78.3%\n",
            "Epoch: 23  D Loss: 1.17969  G Loss: 1.04563  True Positive Rate: 58.3% True Negative Rate: 73.3%\n",
            "Epoch: 24  D Loss: 1.20182  G Loss: 1.00356  True Positive Rate: 73.3% True Negative Rate: 71.7%\n",
            "Epoch: 25  D Loss: 1.1941   G Loss: 1.0219   True Positive Rate: 71.7% True Negative Rate: 78.3%\n",
            "Epoch: 26  D Loss: 1.18125  G Loss: 1.0479   True Positive Rate: 55.0% True Negative Rate: 81.7%\n",
            "Epoch: 27  D Loss: 1.22108  G Loss: 0.977504 True Positive Rate: 51.7% True Negative Rate: 58.3%\n",
            "Epoch: 28  D Loss: 1.23327  G Loss: 0.951166 True Positive Rate: 73.3% True Negative Rate: 75.0%\n",
            "Epoch: 29  D Loss: 1.22825  G Loss: 0.967606 True Positive Rate: 63.3% True Negative Rate: 76.7%\n",
            "Epoch: 30  D Loss: 1.24485  G Loss: 0.938952 True Positive Rate: 55.0% True Negative Rate: 85.0%\n",
            "Epoch: 31  D Loss: 1.24185  G Loss: 0.941264 True Positive Rate: 51.7% True Negative Rate: 71.7%\n",
            "Epoch: 32  D Loss: 1.23348  G Loss: 0.966672 True Positive Rate: 63.3% True Negative Rate: 60.0%\n",
            "Epoch: 33  D Loss: 1.22872  G Loss: 0.962637 True Positive Rate: 58.3% True Negative Rate: 73.3%\n",
            "Epoch: 34  D Loss: 1.25725  G Loss: 0.923148 True Positive Rate: 70.0% True Negative Rate: 78.3%\n",
            "Epoch: 35  D Loss: 1.24875  G Loss: 0.928236 True Positive Rate: 45.0% True Negative Rate: 75.0%\n",
            "Epoch: 36  D Loss: 1.25641  G Loss: 0.920383 True Positive Rate: 65.0% True Negative Rate: 81.7%\n",
            "Epoch: 37  D Loss: 1.25574  G Loss: 0.916105 True Positive Rate: 56.7% True Negative Rate: 76.7%\n",
            "Epoch: 38  D Loss: 1.28397  G Loss: 0.872717 True Positive Rate: 63.3% True Negative Rate: 75.0%\n",
            "Epoch: 39  D Loss: 1.26748  G Loss: 0.902594 True Positive Rate: 51.7% True Negative Rate: 78.3%\n",
            "Epoch: 40  D Loss: 1.26106  G Loss: 0.905036 True Positive Rate: 65.0% True Negative Rate: 70.0%\n",
            "Epoch: 41  D Loss: 1.27182  G Loss: 0.893468 True Positive Rate: 58.3% True Negative Rate: 73.3%\n",
            "Epoch: 42  D Loss: 1.27948  G Loss: 0.879401 True Positive Rate: 58.3% True Negative Rate: 68.3%\n",
            "Epoch: 43  D Loss: 1.28168  G Loss: 0.876719 True Positive Rate: 50.0% True Negative Rate: 55.0%\n",
            "Epoch: 44  D Loss: 1.27288  G Loss: 0.895294 True Positive Rate: 53.3% True Negative Rate: 85.0%\n",
            "Epoch: 45  D Loss: 1.28169  G Loss: 0.877503 True Positive Rate: 63.3% True Negative Rate: 46.7%\n",
            "Epoch: 46  D Loss: 1.27617  G Loss: 0.893499 True Positive Rate: 66.7% True Negative Rate: 66.7%\n",
            "Epoch: 47  D Loss: 1.26979  G Loss: 0.891275 True Positive Rate: 61.7% True Negative Rate: 71.7%\n",
            "Epoch: 48  D Loss: 1.27678  G Loss: 0.890936 True Positive Rate: 75.0% True Negative Rate: 75.0%\n",
            "Epoch: 49  D Loss: 1.28599  G Loss: 0.879489 True Positive Rate: 60.0% True Negative Rate: 68.3%\n",
            "Epoch: 50  D Loss: 1.28364  G Loss: 0.869652 True Positive Rate: 60.0% True Negative Rate: 83.3%\n",
            "Epoch: 51  D Loss: 1.29233  G Loss: 0.863676 True Positive Rate: 48.3% True Negative Rate: 76.7%\n",
            "Epoch: 52  D Loss: 1.28849  G Loss: 0.865757 True Positive Rate: 43.3% True Negative Rate: 70.0%\n",
            "Epoch: 53  D Loss: 1.30082  G Loss: 0.857105 True Positive Rate: 55.0% True Negative Rate: 71.7%\n",
            "Epoch: 54  D Loss: 1.27878  G Loss: 0.888292 True Positive Rate: 61.7% True Negative Rate: 70.0%\n",
            "Epoch: 55  D Loss: 1.29627  G Loss: 0.850382 True Positive Rate: 56.7% True Negative Rate: 58.3%\n",
            "Epoch: 56  D Loss: 1.29771  G Loss: 0.85362  True Positive Rate: 63.3% True Negative Rate: 70.0%\n",
            "Epoch: 57  D Loss: 1.28818  G Loss: 0.865875 True Positive Rate: 58.3% True Negative Rate: 78.3%\n",
            "Epoch: 58  D Loss: 1.29375  G Loss: 0.862107 True Positive Rate: 68.3% True Negative Rate: 76.7%\n",
            "Epoch: 59  D Loss: 1.29699  G Loss: 0.851728 True Positive Rate: 58.3% True Negative Rate: 66.7%\n",
            "Epoch: 60  D Loss: 1.29723  G Loss: 0.85032  True Positive Rate: 65.0% True Negative Rate: 68.3%\n",
            "Epoch: 61  D Loss: 1.29939  G Loss: 0.850245 True Positive Rate: 45.0% True Negative Rate: 80.0%\n",
            "Epoch: 62  D Loss: 1.29578  G Loss: 0.86211  True Positive Rate: 46.7% True Negative Rate: 56.7%\n",
            "Epoch: 63  D Loss: 1.29107  G Loss: 0.863544 True Positive Rate: 53.3% True Negative Rate: 66.7%\n",
            "Epoch: 64  D Loss: 1.30867  G Loss: 0.837711 True Positive Rate: 51.7% True Negative Rate: 71.7%\n",
            "Epoch: 65  D Loss: 1.30516  G Loss: 0.844317 True Positive Rate: 65.0% True Negative Rate: 60.0%\n",
            "Epoch: 66  D Loss: 1.29733  G Loss: 0.857111 True Positive Rate: 65.0% True Negative Rate: 73.3%\n",
            "Epoch: 67  D Loss: 1.30586  G Loss: 0.841411 True Positive Rate: 56.7% True Negative Rate: 75.0%\n",
            "Epoch: 68  D Loss: 1.30824  G Loss: 0.844434 True Positive Rate: 51.7% True Negative Rate: 66.7%\n",
            "Epoch: 69  D Loss: 1.31296  G Loss: 0.831551 True Positive Rate: 58.3% True Negative Rate: 78.3%\n",
            "Epoch: 70  D Loss: 1.30578  G Loss: 0.85137  True Positive Rate: 68.3% True Negative Rate: 53.3%\n",
            "Epoch: 71  D Loss: 1.30862  G Loss: 0.834182 True Positive Rate: 50.0% True Negative Rate: 56.7%\n",
            "Epoch: 72  D Loss: 1.30958  G Loss: 0.834697 True Positive Rate: 51.7% True Negative Rate: 66.7%\n",
            "Epoch: 73  D Loss: 1.31325  G Loss: 0.833448 True Positive Rate: 58.3% True Negative Rate: 71.7%\n",
            "Epoch: 74  D Loss: 1.30991  G Loss: 0.840156 True Positive Rate: 55.0% True Negative Rate: 56.7%\n",
            "Epoch: 75  D Loss: 1.31143  G Loss: 0.837724 True Positive Rate: 60.0% True Negative Rate: 65.0%\n",
            "Epoch: 76  D Loss: 1.31041  G Loss: 0.824752 True Positive Rate: 58.3% True Negative Rate: 58.3%\n",
            "Epoch: 77  D Loss: 1.3079   G Loss: 0.852216 True Positive Rate: 63.3% True Negative Rate: 63.3%\n",
            "Epoch: 78  D Loss: 1.30748  G Loss: 0.836902 True Positive Rate: 55.0% True Negative Rate: 63.3%\n",
            "Epoch: 79  D Loss: 1.30827  G Loss: 0.841221 True Positive Rate: 68.3% True Negative Rate: 63.3%\n",
            "Epoch: 80  D Loss: 1.31875  G Loss: 0.817116 True Positive Rate: 75.0% True Negative Rate: 76.7%\n",
            "Epoch: 81  D Loss: 1.31525  G Loss: 0.83276  True Positive Rate: 55.0% True Negative Rate: 55.0%\n",
            "Epoch: 82  D Loss: 1.31348  G Loss: 0.830226 True Positive Rate: 66.7% True Negative Rate: 53.3%\n",
            "Epoch: 83  D Loss: 1.30896  G Loss: 0.847381 True Positive Rate: 51.7% True Negative Rate: 68.3%\n",
            "Epoch: 84  D Loss: 1.31237  G Loss: 0.827845 True Positive Rate: 78.3% True Negative Rate: 41.7%\n",
            "Epoch: 85  D Loss: 1.31696  G Loss: 0.818082 True Positive Rate: 56.7% True Negative Rate: 66.7%\n",
            "Epoch: 86  D Loss: 1.31813  G Loss: 0.831197 True Positive Rate: 56.7% True Negative Rate: 46.7%\n",
            "Epoch: 87  D Loss: 1.31377  G Loss: 0.825056 True Positive Rate: 56.7% True Negative Rate: 68.3%\n",
            "Epoch: 88  D Loss: 1.31508  G Loss: 0.83175  True Positive Rate: 51.7% True Negative Rate: 78.3%\n",
            "Epoch: 89  D Loss: 1.32184  G Loss: 0.821749 True Positive Rate: 51.7% True Negative Rate: 63.3%\n",
            "Epoch: 90  D Loss: 1.31486  G Loss: 0.830354 True Positive Rate: 61.7% True Negative Rate: 63.3%\n",
            "Epoch: 91  D Loss: 1.31949  G Loss: 0.820482 True Positive Rate: 51.7% True Negative Rate: 78.3%\n",
            "Epoch: 92  D Loss: 1.32333  G Loss: 0.815607 True Positive Rate: 50.0% True Negative Rate: 71.7%\n",
            "Epoch: 93  D Loss: 1.316    G Loss: 0.832061 True Positive Rate: 45.0% True Negative Rate: 73.3%\n",
            "Epoch: 94  D Loss: 1.31326  G Loss: 0.828491 True Positive Rate: 68.3% True Negative Rate: 43.3%\n",
            "Epoch: 95  D Loss: 1.31725  G Loss: 0.831709 True Positive Rate: 65.0% True Negative Rate: 71.7%\n",
            "Epoch: 96  D Loss: 1.31454  G Loss: 0.826634 True Positive Rate: 55.0% True Negative Rate: 73.3%\n",
            "Epoch: 97  D Loss: 1.32301  G Loss: 0.813462 True Positive Rate: 60.0% True Negative Rate: 66.7%\n",
            "Epoch: 98  D Loss: 1.32839  G Loss: 0.819615 True Positive Rate: 58.3% True Negative Rate: 48.3%\n",
            "Epoch: 99  D Loss: 1.31353  G Loss: 0.839977 True Positive Rate: 55.0% True Negative Rate: 66.7%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}